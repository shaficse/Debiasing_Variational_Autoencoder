{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35d8602-2fdd-4305-8cd2-f78bc3ad7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e98cda61-53ae-41f0-8d06-2ab45a20e387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'#'''cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Hyperparameter\n",
    "lr =2e-4\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "logging_interval = 50\n",
    "save_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807edd24-4768-457f-bbbc-bc1d603f7e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our puffer surver we need to browse via a proxy!!\n",
    "import os\n",
    "# # Set HTTP and HTTPS proxy\n",
    "# os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02316bd6",
   "metadata": {},
   "source": [
    "# Code Explanation: Dataset Preparation for MNIST\n",
    "\n",
    "## Transformations\n",
    "### `transforms.Compose`\n",
    "- A utility that allows chaining multiple transformations sequentially.\n",
    "- In this case, the transformation pipeline consists of:\n",
    "  - `transforms.ToTensor()`: Converts a PIL image or NumPy array to a PyTorch tensor and scales pixel values to the range `[0, 1]`.\n",
    "\n",
    "```python\n",
    "trf = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "])\n",
    "- Purpose: Defines the transformation pipeline to be applied to the MNIST images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35151210-5d6d-4564-819d-45510ec1232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:15<00:00, 630kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 148kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:04<00:00, 356kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.36MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trf = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='.',\n",
    "                                train=True,\n",
    "                                transform=trf,\n",
    "                                download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='.',\n",
    "                              train=False,\n",
    "                              transform=trf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0daaa7",
   "metadata": {},
   "source": [
    "- The training and test datasets are loaded with consistent preprocessing using ToTensor.\n",
    "- The train_dataset and test_dataset objects can now be used with a DataLoader for batching, shuffling, and efficient data loading during training and evaluation.\n",
    "- Key Point: Ensure the same transformations are applied to both training and test datasets to avoid discrepancies in data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7b40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ba0c1e8",
   "metadata": {},
   "source": [
    "```\n",
    "dataset=train_dataset: Specifies the training dataset (MNIST in this case) to load data from.\n",
    "batch_size=BATCH_SIZE: Defines the number of samples per batch for training. Ensures that model training proceeds in manageable chunks of data.\n",
    "num_workers=2: Indicates the number of subprocesses to use for data loading. Increasing this value can speed up data loading if hardware supports it.\n",
    "shuffle=True: Ensures that the data is shuffled at every epoch to improve model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9baebf2-7a1f-4171-8850-2d85f55cb601",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=2,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      num_workers=2,\n",
    "                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8a12b0-7198-4354-b14d-d83480aaad8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "\n",
      "Image batch dimensions: torch.Size([256, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "print('Training Set:\\n')\n",
    "for images, labels in train_loader:\n",
    "    print('Image batch dimensions:', images.size())\n",
    "    print('Image label dimensions:', labels.size())\n",
    "    #print(labels[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2ab63",
   "metadata": {},
   "source": [
    "```\n",
    "__init__(self, *args):\n",
    "Accepts the desired shape as arguments and stores it in self.shape.\n",
    "The *args syntax allows flexibility for any number of dimensions.\n",
    "forward(self, x):\n",
    "Reshapes the input tensor x using .view(self.shape).\n",
    "The .view() method changes the shape of the tensor without altering its data.\n",
    "Use Case:\n",
    "Used in neural network architectures where intermediate tensors need to be reshaped to match layer requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349a213d-4bc2-4a58-9ff8-fe025012e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "\n",
    "\n",
    "class Trim(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :28, :28]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b391af",
   "metadata": {},
   "source": [
    "```\n",
    "Explanation:\n",
    "__init__(self, *args):\n",
    "Initializes the module but does not store any parameters.\n",
    "*args is unused but allows flexibility for future extensions.\n",
    "forward(self, x):\n",
    "Crops the input tensor x along its last two dimensions to a size of (28, 28).\n",
    "The slicing x[:, :, :28, :28] keeps all batch and channel dimensions but trims the spatial dimensions to (28, 28).\n",
    "Use Case:\n",
    "Used when ensuring input tensors conform to a specific size (e.g., trimming excess padding or resizing to match model input requirements).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601bce0b-5d79-4a6c-948f-17f5b5cf4883",
   "metadata": {},
   "source": [
    "## Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4519925-2e69-4785-9721-5666278c94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, stride=2, kernel_size=3, bias=False, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.Conv2d(32, 64, stride=2, kernel_size=3, bias=False, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.Conv2d(64, 64, stride=1, kernel_size=3, bias=False, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                #\n",
    "                nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.z_mean = torch.nn.Linear(3136, 2) # 7x7x64\n",
    "        self.z_log_var = torch.nn.Linear(3136, 2)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "                torch.nn.Linear(2, 3136),\n",
    "                Reshape(-1, 64, 7, 7),\n",
    "                #\n",
    "                nn.ConvTranspose2d(64, 64, stride=2, kernel_size=3),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.ConvTranspose2d(64, 64, stride=2, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.ConvTranspose2d(64, 32, stride=2, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.ConvTranspose2d(32, 1, stride=2, kernel_size=3, padding=1),\n",
    "                #\n",
    "                Trim(),  # 1x29x29 -> 1x28x28\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "\n",
    "    def encoding_fn(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "    def reparameterize(self, z_mu, z_log_var):\n",
    "        eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(z_mu.get_device())\n",
    "        z = z_mu + eps * torch.exp(z_log_var/2.)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, z_mean, z_log_var, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450bd197",
   "metadata": {},
   "source": [
    "# Parameter Calculation and Configuration in VAE\n",
    "\n",
    "In the provided VAE implementation, each parameter and layer is carefully calculated to handle the data flow through the network. Below is an explanation of how the parameters are calculated and set, along with the interpretation of their values.\n",
    "\n",
    "---\n",
    "\n",
    "## Input and Feature Map Sizes\n",
    "The input to the model is an image of size `(1, 28, 28)` (MNIST dataset: grayscale images with a single channel).\n",
    "\n",
    "### Encoder\n",
    "The encoder reduces the spatial dimensions and increases feature channels to encode meaningful representations.\n",
    "\n",
    "1. **First Convolution Layer**:\n",
    "   - **Input**: `(1, 28, 28)` (1 channel for grayscale image, 28x28 pixels).\n",
    "   - **Layer**: `nn.Conv2d(1, 32, stride=2, kernel_size=3, padding=1)`\n",
    "     - **Kernel Size**: 3x3 convolutional filters.\n",
    "     - **Stride**: 2 (reduces spatial dimensions by half).\n",
    "     - **Padding**: 1 (ensures output size is computed correctly).\n",
    "     - Output size calculation:\n",
    "       ```\n",
    "       Output Size = [(Input Size + 2 * Padding - Kernel Size) / Stride] + 1\n",
    "       [(28 + 2 * 1 - 3) / 2] + 1 = 14\n",
    "       ```\n",
    "     - **Output**: `(32, 14, 14)` (32 feature channels).\n",
    "\n",
    "2. **Second Convolution Layer**:\n",
    "   - **Input**: `(32, 14, 14)`\n",
    "   - **Layer**: `nn.Conv2d(32, 64, stride=2, kernel_size=3, padding=1)`\n",
    "     - **Output Size Calculation**:\n",
    "       ```\n",
    "       [(14 + 2 * 1 - 3) / 2] + 1 = 7\n",
    "       ```\n",
    "     - **Output**: `(64, 7, 7)` (64 feature channels).\n",
    "\n",
    "3. **Third Convolution Layer**:\n",
    "   - **Input**: `(64, 7, 7)`\n",
    "   - **Layer**: `nn.Conv2d(64, 64, stride=1, kernel_size=3, padding=1)`\n",
    "     - **Output Size Calculation**:\n",
    "       ```\n",
    "       [(7 + 2 * 1 - 3) / 1] + 1 = 7\n",
    "       ```\n",
    "     - **Output**: `(64, 7, 7)` (same size, 64 feature channels).\n",
    "\n",
    "4. **Flatten Layer**:\n",
    "   - **Input**: `(64, 7, 7)`\n",
    "   - **Flattened Size**:\n",
    "     ```\n",
    "     64 * 7 * 7 = 3136\n",
    "     ```\n",
    "   - **Output**: `(3136)` (flattened vector for fully connected layers).\n",
    "\n",
    "5. **Latent Space Parameters**:\n",
    "   - **z_mean**: Linear layer maps the flattened size `(3136)` to the latent space dimension `(2)`. Represents the mean of the latent distribution.\n",
    "   - **z_log_var**: Linear layer maps the flattened size `(3136)` to `(2)`. Represents the log variance of the latent distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### Decoder\n",
    "The decoder reconstructs the input image from the latent space.\n",
    "\n",
    "1. **First Linear Layer**:\n",
    "   - **Input**: Latent space `(2)` (compressed representation of the image).\n",
    "   - **Layer**: `torch.nn.Linear(2, 3136)`\n",
    "   - **Output**: Flattened size `(3136)`.\n",
    "\n",
    "2. **Reshape Layer**:\n",
    "   - **Input**: `(3136)`\n",
    "   - Reshaped to `(64, 7, 7)` to match the encoder's output dimensions.\n",
    "\n",
    "3. **First Transposed Convolution Layer**:\n",
    "   - **Input**: `(64, 7, 7)`\n",
    "   - **Layer**: `nn.ConvTranspose2d(64, 64, stride=2, kernel_size=3)`\n",
    "     - **Output Size Calculation**:\n",
    "       ```\n",
    "       (7 - 1) * 2 + 3 = 15\n",
    "       ```\n",
    "     - **Output**: `(64, 15, 15)`.\n",
    "\n",
    "4. **Second Transposed Convolution Layer**:\n",
    "   - **Input**: `(64, 15, 15)`\n",
    "   - **Layer**: `nn.ConvTranspose2d(64, 64, stride=2, kernel_size=3, padding=1)`\n",
    "     - **Output Size Calculation**:\n",
    "       ```\n",
    "       (15 - 1) * 2 - 2 * 1 + 3 = 30\n",
    "       ```\n",
    "     - **Output**: `(64, 30, 30)`.\n",
    "\n",
    "5. **Third Transposed Convolution Layer**:\n",
    "   - **Input**: `(64, 30, 30)`\n",
    "   - **Layer**: `nn.ConvTranspose2d(64, 32, stride=2, kernel_size=3, padding=1)`\n",
    "     - **Output Size Calculation**:\n",
    "       ```\n",
    "       (30 - 1) * 2 - 2 * 1 + 3 = 60\n",
    "       ```\n",
    "     - **Output**: `(32, 60, 60)`.\n",
    "\n",
    "6. **Fourth Transposed Convolution Layer**:\n",
    "   - **Input**: `(32, 60, 60)`\n",
    "   - **Layer**: `nn.ConvTranspose2d(32, 1, stride=2, kernel_size=3, padding=1)`\n",
    "     - **Output Size Calculation**:\n",
    "       ```\n",
    "       (60 - 1) * 2 - 2 * 1 + 3 = 121\n",
    "       ```\n",
    "     - **Output**: `(1, 121, 121)`.\n",
    "\n",
    "7. **Trim Layer**:\n",
    "   - Crops the output from `(1, 121, 121)` to `(1, 28, 28)` to match the input dimensions.\n",
    "\n",
    "8. **Sigmoid Activation**:\n",
    "   - Normalizes pixel values to the range `[0, 1]`.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Representations\n",
    "- **Input (Encoder)**:\n",
    "  - Initial size: `(1, 28, 28)` (grayscale MNIST image).\n",
    "  - Compressed to latent representation `(2)` using convolutional and linear layers.\n",
    "\n",
    "- **Latent Space**:\n",
    "  - **z_mean**: Represents the mean of the latent distribution (learned by the encoder).\n",
    "  - **z_log_var**: Represents the log variance of the latent distribution.\n",
    "\n",
    "- **Output (Decoder)**:\n",
    "  - Expands the latent vector `(2)` back to the original input size `(1, 28, 28)` using transposed convolutions.\n",
    "  - Final output represents the reconstructed image with normalized pixel values.\n",
    "\n",
    "- Each layer's parameters are chosen to ensure smooth dimensionality reduction (encoder) and reconstruction (decoder), maintaining compatibility between layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6b76fe-1718-4ab5-bb11-2bc449bb9ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    (3): Dropout2d(p=0.25, inplace=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    (7): Dropout2d(p=0.25, inplace=False)\n",
       "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    (11): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (z_mean): Linear(in_features=3136, out_features=2, bias=True)\n",
       "  (z_log_var): Linear(in_features=3136, out_features=2, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=3136, bias=True)\n",
       "    (1): Reshape()\n",
       "    (2): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    (5): Dropout2d(p=0.25, inplace=False)\n",
       "    (6): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    (9): Dropout2d(p=0.25, inplace=False)\n",
       "    (10): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    (13): Dropout2d(p=0.25, inplace=False)\n",
       "    (14): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (15): Trim()\n",
       "    (16): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56679385-7955-4ece-ab19-896040a10ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num. of parametes: 170789\n",
      "Total num. of Trainable parametes: 170789\n"
     ]
    }
   ],
   "source": [
    "# Total num. of parametes\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "# Total num. of \"trainable\" parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total num. of parametes: {num_params}')\n",
    "print(f'Total num. of Trainable parametes: {num_trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f85e1f75-a14c-4af7-a809-387df43bebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.mse_loss # evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f395b5b0-cfc2-4a8d-bf1e-5b2b6f6e8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epoch_loss_autoencoder(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    curr_loss, num_examples = 0., 0\n",
    "    with torch.no_grad():\n",
    "        for features, _ in data_loader:\n",
    "            features = features.to(device)\n",
    "            _, _, _, logits = model(features)\n",
    "            loss = loss_fn(logits, features, reduction='sum')\n",
    "            num_examples += features.size(0)\n",
    "            curr_loss += loss\n",
    "\n",
    "        curr_loss = curr_loss / num_examples\n",
    "        return curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "348ef2a7-7f46-4175-b09b-b7006f735daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82011838-5258-4778-83f5-7fd416bca16d",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b67577-723a-4064-bb53-847b7f6469f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = {'train_combined_loss_per_batch': [],\n",
    "            'train_combined_loss_per_epoch': [],\n",
    "            'train_reconstruction_loss_per_batch': [],\n",
    "            'train_kl_loss_per_batch': []}\n",
    "\n",
    "skip_epoch_stats = False\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  model.train()\n",
    "  for batch_idx, (features, _) in enumerate(train_loader):\n",
    "\n",
    "    features = features.to(device)\n",
    "\n",
    "    # FORWARD AND BACK PROP\n",
    "    encoded, z_mean, z_log_var, decoded = model(features)\n",
    "\n",
    "    # total loss = reconstruction loss + KL divergence\n",
    "    # kl_divergence = (0.5 * (z_mean**2 +\n",
    "    #                         torch.exp(z_log_var) - z_log_var - 1)).sum()\n",
    "    kl_div = -0.5 * torch.sum(1 + z_log_var\n",
    "                              - z_mean**2\n",
    "                              - torch.exp(z_log_var),\n",
    "                              axis=1) # sum over latent dimension\n",
    "\n",
    "    batchsize = kl_div.size(0)\n",
    "    kl_div = kl_div.mean() # average over batch dimension\n",
    "\n",
    "    pixelwise = loss_fn(decoded, features, reduction='none')\n",
    "    pixelwise = pixelwise.view(batchsize, -1).sum(axis=1) # sum over pixels\n",
    "    pixelwise = pixelwise.mean() # average over batch dimension\n",
    "\n",
    "    loss = pixelwise + kl_div\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    # UPDATE MODEL PARAMETERS\n",
    "    optimizer.step()\n",
    "\n",
    "    # LOGGING\n",
    "    log_dict['train_combined_loss_per_batch'].append(loss.item())\n",
    "    log_dict['train_reconstruction_loss_per_batch'].append(pixelwise.item())\n",
    "    log_dict['train_kl_loss_per_batch'].append(kl_div.item())\n",
    "\n",
    "    if not batch_idx % logging_interval:\n",
    "        print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n",
    "              % (epoch+1, NUM_EPOCHS, batch_idx,\n",
    "                  len(train_loader), loss))\n",
    "\n",
    "  if not skip_epoch_stats:\n",
    "      model.eval()\n",
    "\n",
    "      with torch.set_grad_enabled(False):  # save memory during inference\n",
    "          train_loss = compute_epoch_loss_autoencoder(\n",
    "              model, train_loader, loss_fn, device)\n",
    "          print('***Epoch: %03d/%03d | Loss: %.3f' % (\n",
    "                epoch+1, NUM_EPOCHS, train_loss))\n",
    "          log_dict['train_combined_loss_per_epoch'].append(train_loss.item())\n",
    "\n",
    "  print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "if save_model is not None:\n",
    "    torch.save({\n",
    "          'model_state_dict': model.state_dict(),\n",
    "          'optimizer_state_dict': optimizer_D.state_dict()\n",
    "          }, 'vae_mnist.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd7193c-084a-4557-86df-6f2c988f1a6d",
   "metadata": {},
   "source": [
    "## Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5741146-695c-4f34-865f-00caeefa30d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(minibatch_losses, num_epochs, averaging_iterations=100, custom_label=''):\n",
    "\n",
    "    iter_per_epoch = len(minibatch_losses) // num_epochs\n",
    "\n",
    "    plt.figure()\n",
    "    ax1 = plt.subplot(1, 1, 1)\n",
    "    ax1.plot(range(len(minibatch_losses)),\n",
    "             (minibatch_losses), label=f'Minibatch Loss{custom_label}')\n",
    "    ax1.set_xlabel('Iterations')\n",
    "    ax1.set_ylabel('Loss')\n",
    "\n",
    "    if len(minibatch_losses) < 1000:\n",
    "        num_losses = len(minibatch_losses) // 2\n",
    "    else:\n",
    "        num_losses = 1000\n",
    "\n",
    "    ax1.set_ylim([\n",
    "        0, np.max(minibatch_losses[num_losses:])*1.5\n",
    "        ])\n",
    "\n",
    "    ax1.plot(np.convolve(minibatch_losses,\n",
    "                         np.ones(averaging_iterations,)/averaging_iterations,\n",
    "                         mode='valid'),\n",
    "             label=f'Running Average{custom_label}')\n",
    "    ax1.legend()\n",
    "\n",
    "    ###################\n",
    "    # Set scond x-axis\n",
    "    ax2 = ax1.twiny()\n",
    "    newlabel = list(range(num_epochs+1))\n",
    "\n",
    "    newpos = [e*iter_per_epoch for e in newlabel]\n",
    "\n",
    "    ax2.set_xticks(newpos[::10])\n",
    "    ax2.set_xticklabels(newlabel[::10])\n",
    "\n",
    "    ax2.xaxis.set_ticks_position('bottom')\n",
    "    ax2.xaxis.set_label_position('bottom')\n",
    "    ax2.spines['bottom'].set_position(('outward', 45))\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_xlim(ax1.get_xlim())\n",
    "    ###################\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d874cfc-b7e3-4bfc-82aa-6e9cf54952e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(log_dict['train_reconstruction_loss_per_batch'], NUM_EPOCHS, custom_label=\" (reconstruction)\")\n",
    "plot_training_loss(log_dict['train_kl_loss_per_batch'], NUM_EPOCHS, custom_label=\" (KL)\")\n",
    "plot_training_loss(log_dict['train_combined_loss_per_batch'], NUM_EPOCHS, custom_label=\" (combined)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899d132-f9d7-449f-bd66-402c2f95ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(data_loader, model, device,\n",
    "                          unnormalizer=None,\n",
    "                          figsize=(20, 2.5), n_images=15):\n",
    "\n",
    "  fig, axes = plt.subplots(nrows=2, ncols=n_images,\n",
    "                            sharex=True, sharey=True, figsize=figsize)\n",
    "\n",
    "  for batch_idx, (features, _) in enumerate(data_loader):\n",
    "\n",
    "    features = features.to(device)\n",
    "\n",
    "    color_channels = features.shape[1]\n",
    "    image_height = features.shape[2]\n",
    "    image_width = features.shape[3]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded, z_mean, z_log_var, decoded_images = model(features)[:n_images]\n",
    "\n",
    "    orig_images = features[:n_images]\n",
    "    break\n",
    "\n",
    "  for i in range(n_images):\n",
    "    for ax, img in zip(axes, [orig_images, decoded_images]):\n",
    "      curr_img = img[i].detach().to(torch.device('cpu'))\n",
    "      if unnormalizer is not None:\n",
    "        curr_img = unnormalizer(curr_img)\n",
    "\n",
    "      if color_channels > 1:\n",
    "        curr_img = np.transpose(curr_img, (1, 2, 0))\n",
    "        ax[i].imshow(curr_img)\n",
    "      else:\n",
    "        ax[i].imshow(curr_img.view((image_height, image_width)), cmap='binary')\n",
    "\n",
    "\n",
    "def plot_latent_space_with_labels(num_classes, data_loader, encoding_fn, device):\n",
    "  d = {i:[] for i in range(num_classes)}\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "      features = features.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      embedding = encoding_fn(features)\n",
    "\n",
    "      for i in range(num_classes):\n",
    "        if i in targets:\n",
    "          mask = targets == i\n",
    "          d[i].append(embedding[mask].to('cpu').numpy())\n",
    "\n",
    "  colors = list(mcolors.TABLEAU_COLORS.items())\n",
    "  for i in range(num_classes):\n",
    "    d[i] = np.concatenate(d[i])\n",
    "    plt.scatter(\n",
    "        d[i][:, 0], d[i][:, 1],\n",
    "        color=colors[i][1],\n",
    "        label=f'{i}',\n",
    "        alpha=0.5)\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "\n",
    "def plot_images_sampled_from_vae(model, device, latent_size, unnormalizer=None, num_images=10):\n",
    "\n",
    "  with torch.no_grad():\n",
    "    ##########################\n",
    "    ### RANDOM SAMPLE\n",
    "    ##########################\n",
    "\n",
    "    rand_features = torch.randn(num_images, latent_size).to(device)\n",
    "    new_images = model.decoder(rand_features)\n",
    "    color_channels = new_images.shape[1]\n",
    "    image_height = new_images.shape[2]\n",
    "    image_width = new_images.shape[3]\n",
    "\n",
    "    ##########################\n",
    "    ### VISUALIZATION\n",
    "    ##########################\n",
    "\n",
    "    image_width = 28\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_images, figsize=(10, 2.5), sharey=True)\n",
    "    decoded_images = new_images[:num_images]\n",
    "\n",
    "    for ax, img in zip(axes, decoded_images):\n",
    "      curr_img = img.detach().to(torch.device('cpu'))\n",
    "      if unnormalizer is not None:\n",
    "        curr_img = unnormalizer(curr_img)\n",
    "\n",
    "      if color_channels > 1:\n",
    "        curr_img = np.transpose(curr_img, (1, 2, 0))\n",
    "        ax.imshow(curr_img)\n",
    "      else:\n",
    "        ax.imshow(curr_img.view((image_height, image_width)), cmap='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd3b5f-42c9-4424-944e-930c890c0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_generated_images(data_loader=train_loader, model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4c446-9e21-479c-abd3-33df54ea1203",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space_with_labels(\n",
    "    num_classes=10,\n",
    "    data_loader=train_loader,\n",
    "    encoding_fn=model.encoding_fn,\n",
    "    device=device)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e1aa24-fde8-48fe-b4a0-8e5cf90042d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plot_images_sampled_from_vae(model=model, device=device, latent_size=2)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis-det",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
